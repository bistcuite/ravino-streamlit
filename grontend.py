import google.generativeai as genai
import streamlit as st

# ØªÙ†Ø¸ÛŒÙ… Ú©Ù„ÛŒØ¯ API Ùˆ Ù…Ø¯Ù„
genai.configure(api_key="AIzaSyDuExne7oG9NnfNZeaqDOSXVtxUdau7IBU")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„
generation_config = {
    "temperature": 0.25,
    "top_p": 0.55,
    "top_k": 64,
    "max_output_tokens": 8192,
    "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash-exp",
    generation_config=generation_config,
  system_instruction="""Ø´Ù…Ø§ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¢Ù…ÙˆØ²Ø´Ú¯Ø§Ù‡ Ø²Ø¨Ø§Ù† Â«Ø²Ø¨Ø§Ù†Ø²Ø¯Â» Ù‡Ø³ØªÛŒØ¯. Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª Ø¢Ù…ÙˆØ²Ø´Ú¯Ø§Ù‡: https://zabanzadacademy.ir/

Ø¯Ùˆ ÙˆØ¸ÛŒÙÙ‡ Ø§ØµÙ„ÛŒ Ø¯Ø§Ø±ÛŒØ¯:

---

ğŸŸ¦ Û±. Ù…Ø´Ø§ÙˆØ±Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´Ú¯Ø§Ù‡:

Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø®ØªØ§Ø±Ù…Ù†Ø¯ Ø¯Ø± Ù‚Ø§Ù„Ø¨ JSON Ø²ÛŒØ± Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:

ğŸ“¦ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§:
```json
{
  "Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§": [
    {
      "Ù†Ø§Ù…": "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø²Ø¨Ø§Ù† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ - Ø³Ø·Ø­ Ù…ØªÙˆØ³Ø·",
      "Ø³Ø·Ø­": "Ù…ØªÙˆØ³Ø·",
      "ØªÙˆØ¶ÛŒØ­": "ØªÙ‚ÙˆÛŒØª Ù…Ù‡Ø§Ø±Øª Ù…Ú©Ø§Ù„Ù…Ù‡ Ùˆ Ø´Ù†ÛŒØ¯Ø§Ø±ÛŒ Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ù…Ø±Ù‡",
      "Ù…Ø¯Ø±Ø³": "Ø³Ø±Ú©Ø§Ø± Ø®Ø§Ù†Ù… Ø³Ø§Ø±Ø§ Ø¬Ø§Ù†Ø³ÙˆÙ†",
      "Ù‚ÛŒÙ…Øª": 1200000,
      "ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø§Øª": 12,
      "Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ": "Ø¯ÙˆØ´Ù†Ø¨Ù‡ Ùˆ Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡â€ŒÙ‡Ø§ - Ø³Ø§Ø¹Øª Û±Û¸ ØªØ§ Û±Û¹:Û³Û°",
      "Ø¸Ø±ÙÛŒØª Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡": 5
    },
    {
      "Ù†Ø§Ù…": "Ø¯ÙˆØ±Ù‡ Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ø¢ÛŒÙ„ØªØ³",
      "Ø³Ø·Ø­": "Ù¾ÛŒØ´Ø±ÙØªÙ‡",
      "ØªÙˆØ¶ÛŒØ­": "Ø¢Ù…Ø§Ø¯Ú¯ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† Ø¢ÛŒÙ„ØªØ³ Ø¢Ú©Ø§Ø¯Ù…ÛŒÚ© Ùˆ Ø¬Ù†Ø±Ø§Ù„",
      "Ù…Ø¯Ø±Ø³": "Ø¢Ù‚Ø§ÛŒ Ø¹Ù„ÛŒ Ø­Ø³ÛŒÙ†ÛŒ",
      "Ù‚ÛŒÙ…Øª": 1800000,
      "ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø§Øª": 15,
      "Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ": "Ø³Ù‡â€ŒØ´Ù†Ø¨Ù‡ Ùˆ Ù¾Ù†Ø¬â€ŒØ´Ù†Ø¨Ù‡â€ŒÙ‡Ø§ - Ø³Ø§Ø¹Øª Û±Û· ØªØ§ Û±Û¹",
      "Ø¸Ø±ÙÛŒØª Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡": 2
    },
    {
      "Ù†Ø§Ù…": "Ú¯Ø±Ø§Ù…Ø± Ùˆ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø³Ø·Ø­ Ù…Ù‚Ø¯Ù…Ø§ØªÛŒ",
      "Ø³Ø·Ø­": "Ù…Ø¨ØªØ¯ÛŒ",
      "ØªÙˆØ¶ÛŒØ­": "Ø¢Ù…ÙˆØ²Ø´ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ú¯Ø±Ø§Ù…Ø± Ùˆ Ù„ØºØ§Øª Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø§ÙˆÙ„ÛŒÙ‡",
      "Ù…Ø¯Ø±Ø³": "Ø³Ø±Ú©Ø§Ø± Ø®Ø§Ù†Ù… Ù„Ø§Ø¯Ù† Ù…Ù‡Ø±",
      "Ù‚ÛŒÙ…Øª": 900000,
      "ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø§Øª": 10,
      "Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ": "Ø´Ù†Ø¨Ù‡â€ŒÙ‡Ø§ - Ø³Ø§Ø¹Øª Û±Û° ØªØ§ Û±Û²",
      "Ø¸Ø±ÙÛŒØª Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡": 8
    },
    {
      "Ù†Ø§Ù…": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ¬Ø§Ø±ÛŒ",
      "Ø³Ø·Ø­": "Ø¨Ø§Ù„Ø§-Ù…ØªÙˆØ³Ø·",
      "ØªÙˆØ¶ÛŒØ­": "Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø¬Ù„Ø³Ø§ØªØŒ Ù¾Ø±Ø²Ù†ØªÛŒØ´Ù† Ùˆ Ù…Ú©Ø§ØªØ¨Ø§Øª ØªØ¬Ø§Ø±ÛŒ",
      "Ù…Ø¯Ø±Ø³": "Ø¢Ù‚Ø§ÛŒ Ø¬ÛŒÙ…Ø² Ú©Ø§Ø±ØªØ±",
      "Ù‚ÛŒÙ…Øª": 1500000,
      "ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø§Øª": 10,
      "Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ": "Ø¬Ù…Ø¹Ù‡â€ŒÙ‡Ø§ - Ø³Ø§Ø¹Øª Û±Û´ ØªØ§ Û±Û·",
      "Ø¸Ø±ÙÛŒØª Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡": 0
    }
  ]
}
""",
)
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡ Streamlit
st.set_page_config(page_title="Ø§Ø² Ø²Ø¨Ø§Ù†Ø²Ø¯ Ø¨Ù¾Ø±Ø³")
st.markdown("""
<link href="https://cdn.jsdelivr.net/gh/rastikerdar/vazirmatn@v33.003/Vazirmatn-font-face.css" rel="stylesheet" type="text/css" />
<style>
body, html {
    direction: RTL;
    unicode-bidi: bidi-override;
    text-align: right;
    font-family: Vazirmatn, sans-serif!important;
}
p, div, input, label, h1, h2, h3, h4, h5, h6 {
    direction: RTL;
    text-align: right;
    font-family: Vazirmatn, sans-serif!important;
}
</style>
""", unsafe_allow_html=True)

st.markdown("""
    <style>
        .stTextInput {
        position: fixed;
        bottom: 0;
        padding-bottom: 45px;
        padding-right: 20px;
        padding-left: 20px;
        right: 0;
        left: 0;
        width: 100%;
        margin-left: 1rem;
            z-index:100;
      }
    </style>
""", unsafe_allow_html=True)

# Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ¶Ø¹ÛŒØª (state) Ø¨Ø±Ù†Ø§Ù…Ù‡
if 'prompt' not in st.session_state:
    st.session_state.prompt = ""

if 'chat_session' not in st.session_state:
    st.session_state.chat_session = model.start_chat(history=[])

if 'i' not in st.session_state:
    st.session_state.i = 0

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…
def submit():
    st.session_state.prompt = st.session_state.user_input
    st.session_state.user_input = ""

# Ø¨Ø®Ø´ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
st.text_input("", placeholder="Ú†ÛŒØ²ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯", on_change=submit, key="user_input")

# Ø´Ø±ÙˆØ¹ Ø¯Ø§Ø³ØªØ§Ù† (ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
if 'started' not in st.session_state:
    response = st.session_state.chat_session.send_message("Ø´Ø±ÙˆØ¹ Ù…Ú©Ø§Ù„Ù…Ù‡")
    st.session_state.started = True
    st.session_state.initial_response = response.text

# Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡
st.title("Ø²Ø¨Ø§Ù†â€ŒÙ…Ù†Ø¯")
st.markdown(st.session_state.initial_response)

# st.write(st.session_state.initial_response)
# Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª (Ø¨Ù‡ Ø¬Ø² Ù¾ÛŒØ§Ù… Ø§ÙˆÙ„ÛŒÙ‡)

while st.session_state.i < len(st.session_state.chat_session.history):
    message = st.session_state.chat_session.history[st.session_state.i]

    if message.role == "user":
        if message.parts[0].text == "Ø´Ø±ÙˆØ¹ Ù…Ú©Ø§Ù„Ù…Ù‡":
            st.session_state.i += 1
            continue
        with st.chat_message("user"):
            st.write(message.parts[0].text)
    else:
        if message.parts[0].text != st.session_state.initial_response:
            st.write(message.parts[0].text)
        else:
            st.session_state.i += 1
            continue
    st.session_state.i += 1
    

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
if st.session_state.prompt and 'started' in st.session_state:
    # Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù…Ø¯Ù„
    response = st.session_state.chat_session.send_message(st.session_state.prompt)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
    with st.chat_message("user"):
        st.write(st.session_state.prompt)
    
    # Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
    st.write(response.text)
    print(st.session_state.chat_session.history)
    # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù¾Ø³ Ø§Ø² Ø§Ø±Ø³Ø§Ù„
    st.session_state.prompt = ""

# import time
# import os
# import joblib
# import streamlit as st
# import google.generativeai as genai
# from dotenv import load_dotenv
# load_dotenv()
# genai.configure(api_key="AIzaSyDuExne7oG9NnfNZeaqDOSXVtxUdau7IBU")

# new_chat_id = f'{time.time()}'
# MODEL_ROLE = 'ai'
# AI_AVATAR_ICON = 'âœ¨'

# # Create a data/ folder if it doesn't already exist
# try:
#     os.mkdir('data/')
# except:
#     # data/ folder already exists
#     pass

# # Load past chats (if available)
# try:
#     past_chats: dict = joblib.load('data/past_chats_list')
# except:
#     past_chats = {}

# # Sidebar allows a list of past chats
# with st.sidebar:
#     st.write('# Past Chats')
#     if st.session_state.get('chat_id') is None:
#         st.session_state.chat_id = st.selectbox(
#             label='Pick a past chat',
#             options=[new_chat_id] + list(past_chats.keys()),
#             format_func=lambda x: past_chats.get(x, 'New Chat'),
#             placeholder='_',
#         )
#     else:
#         # This will happen the first time AI response comes in
#         st.session_state.chat_id = st.selectbox(
#             label='Pick a past chat',
#             options=[new_chat_id, st.session_state.chat_id] + list(past_chats.keys()),
#             index=1,
#             format_func=lambda x: past_chats.get(x, 'New Chat' if x != st.session_state.chat_id else st.session_state.chat_title),
#             placeholder='_',
#         )
#     # Save new chats after a message has been sent to AI
#     # TODO: Give user a chance to name chat
#     st.session_state.chat_title = f'ChatSession-{st.session_state.chat_id}'

# st.write('# Chat with Gemini')

# # Chat history (allows to ask multiple questions)
# try:
#     st.session_state.messages = joblib.load(
#         f'data/{st.session_state.chat_id}-st_messages'
#     )
#     st.session_state.gemini_history = joblib.load(
#         f'data/{st.session_state.chat_id}-gemini_messages'
#     )
#     print('old cache')
# except:
#     st.session_state.messages = []
#     st.session_state.gemini_history = []
#     print('new_cache made')
# st.session_state.model = genai.GenerativeModel('gemini-pro')
# st.session_state.chat = st.session_state.model.start_chat(
#     history=st.session_state.gemini_history,
# )

# # Display chat messages from history on app rerun
# for message in st.session_state.messages:
#     with st.chat_message(
#         name=message['role'],
#         avatar=message.get('avatar'),
#     ):
#         st.markdown(message['content'])

# # React to user input
# if prompt := st.chat_input('Your message here...'):
#     # Save this as a chat for later
#     if st.session_state.chat_id not in past_chats.keys():
#         past_chats[st.session_state.chat_id] = st.session_state.chat_title
#         joblib.dump(past_chats, 'data/past_chats_list')
#     # Display user message in chat message container
#     with st.chat_message('user'):
#         st.markdown(prompt)
#     # Add user message to chat history
#     st.session_state.messages.append(
#         dict(
#             role='user',
#             content=prompt,
#         )
#     )
#     ## Send message to AI
#     response = st.session_state.chat.send_message(
#         prompt,
#         stream=True,
#     )
#     # Display assistant response in chat message container
#     with st.chat_message(
#         name=MODEL_ROLE,
#         avatar=AI_AVATAR_ICON,
#     ):
#         message_placeholder = st.empty()
#         full_response = ''
#         assistant_response = response
#         # Streams in a chunk at a time
#         for chunk in response:
#             # Simulate stream of chunk
#             # TODO: Chunk missing `text` if API stops mid-stream ("safety"?)
#             for ch in chunk.text.split(' '):
#                 full_response += ch + ' '
#                 time.sleep(0.05)
#                 # Rewrites with a cursor at end
#                 message_placeholder.write(full_response + 'â–Œ')
#         # Write full message with placeholder
#         message_placeholder.write(full_response)

#     # Add assistant response to chat history
#     st.session_state.messages.append(
#         dict(
#             role=MODEL_ROLE,
#             content=st.session_state.chat.history[-1].parts[0].text,
#             avatar=AI_AVATAR_ICON,
#         )
#     )
#     st.session_state.gemini_history = st.session_state.chat.history
#     # Save to file
#     joblib.dump(
#         st.session_state.messages,
#         f'data/{st.session_state.chat_id}-st_messages',
#     )
#     joblib.dump(
#         st.session_state.gemini_history,
#         f'data/{st.session_state.chat_id}-gemini_messages',
#     )
